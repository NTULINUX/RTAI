From 3e14bcc4dc8e570e954c1ba3caf488f650fac205 Mon Sep 17 00:00:00 2001
From: Alec Ari <neotheuser@ymail.com>
Date: Tue, 2 Jul 2019 22:16:33 -0500
Subject: Convert to RTAI

Signed-off-by: Alec Ari <neotheuser@ymail.com>
---
 arch/x86/kernel/ipipe.c     |  6 ---
 include/ipipe/thread_info.h |  6 ++-
 kernel/ipipe/core.c         | 87 +++++++++++++------------------------
 kernel/ipipe/timer.c        | 10 +++--
 kernel/sched/core.c         | 71 +++++++++++++++++-------------
 5 files changed, 81 insertions(+), 99 deletions(-)

diff --git a/arch/x86/kernel/ipipe.c b/arch/x86/kernel/ipipe.c
index e89d05553877..54afd07fe9df 100644
--- a/arch/x86/kernel/ipipe.c
+++ b/arch/x86/kernel/ipipe.c
@@ -264,15 +264,9 @@ EXPORT_SYMBOL_GPL(ipipe_set_irq_affinity);
 
 void ipipe_send_ipi(unsigned int ipi, cpumask_t cpumask)
 {
-	unsigned long flags;
-
-	flags = hard_local_irq_save();
-
 	cpumask_clear_cpu(ipipe_processor_id(), &cpumask);
 	if (likely(!cpumask_empty(&cpumask)))
 		apic->send_IPI_mask(&cpumask, ipipe_apic_irq_vector(ipi));
-
-	hard_local_irq_restore(flags);
 }
 EXPORT_SYMBOL_GPL(ipipe_send_ipi);
 
diff --git a/include/ipipe/thread_info.h b/include/ipipe/thread_info.h
index 7038c12942c8..1461bfc29a0a 100644
--- a/include/ipipe/thread_info.h
+++ b/include/ipipe/thread_info.h
@@ -7,8 +7,12 @@
  */
 
 struct ipipe_threadinfo {
+	void *ptd[4];
 };
 
-#define __ipipe_init_threadinfo(__p) do { } while (0)
+static inline void __ipipe_init_threadinfo(struct ipipe_threadinfo *p)
+{
+	p->ptd[0] = p->ptd[1] = p->ptd[2] = p->ptd[3] = 0;
+}
 
 #endif /* !_IPIPE_THREAD_INFO_H */
diff --git a/kernel/ipipe/core.c b/kernel/ipipe/core.c
index ec751abeca53..fabf96473b12 100644
--- a/kernel/ipipe/core.c
+++ b/kernel/ipipe/core.c
@@ -42,6 +42,20 @@
 #include <asm/syscall.h>
 #include <asm/unistd.h>
 
+EXPORT_SYMBOL(sys_call_table);
+int (*rtai_fastcall_hook)(struct pt_regs *) = NULL;
+EXPORT_SYMBOL(rtai_fastcall_hook);
+int (*rtai_syscall_hook)(struct pt_regs *) = NULL;
+EXPORT_SYMBOL(rtai_syscall_hook);
+int (*rtai_trap_hook)(int exception, struct pt_regs *) = NULL;
+EXPORT_SYMBOL(rtai_trap_hook);
+int (*rtai_kevent_hook)(int kevent, void *) = NULL;
+EXPORT_SYMBOL(rtai_kevent_hook);
+void (*rtai_migration_hook)(struct task_struct *) = NULL;
+EXPORT_SYMBOL(rtai_migration_hook);
+void (*dispatch_irq_head)(unsigned int) = NULL;
+EXPORT_SYMBOL(dispatch_irq_head);
+
 struct ipipe_domain ipipe_root;
 EXPORT_SYMBOL_GPL(ipipe_root);
 
@@ -990,14 +1004,14 @@ void ipipe_set_hooks(struct ipipe_domain *ipd, int enables)
 }
 EXPORT_SYMBOL_GPL(ipipe_set_hooks);
 
-int __weak ipipe_fastcall_hook(struct pt_regs *regs)
+int ipipe_fastcall_hook(struct pt_regs *regs)
 {
-	return -1;	/* i.e. fall back to slow path. */
+	return rtai_fastcall_hook ? rtai_fastcall_hook(regs) : -1;
 }
 
-int __weak ipipe_syscall_hook(struct ipipe_domain *ipd, struct pt_regs *regs)
+int __ipipe_notify_syscall(struct pt_regs *regs)
 {
-	return 0;
+	return rtai_syscall_hook ? rtai_syscall_hook(regs) : 0;
 }
 
 static inline void sync_root_irqs(void)
@@ -1077,7 +1091,7 @@ int ipipe_handle_syscall(struct thread_info *ti,
 	return 0; /* pass syscall down to the host. */
 }
 
-int __ipipe_notify_syscall(struct pt_regs *regs)
+int __Ipipe_notify_syscall(struct pt_regs *regs)
 {
 	struct ipipe_domain *caller_domain, *this_domain, *ipd;
 	struct ipipe_percpu_domain_data *p;
@@ -1098,7 +1112,6 @@ int __ipipe_notify_syscall(struct pt_regs *regs)
 		__ipipe_set_current_context(p);
 		p->coflags |= __IPIPE_SYSCALL_R;
 		hard_local_irq_restore(flags);
-		ret = ipipe_syscall_hook(caller_domain, regs);
 		flags = hard_local_irq_save();
 		p->coflags &= ~__IPIPE_SYSCALL_R;
 		if (__ipipe_current_domain != ipd)
@@ -1128,12 +1141,12 @@ int __ipipe_notify_syscall(struct pt_regs *regs)
 	return ret;
 }
 
-int __weak ipipe_trap_hook(struct ipipe_trap_data *data)
+int __ipipe_notify_trap(int exception, struct pt_regs *regs)
 {
-	return 0;
+	return (!__ipipe_root_p && rtai_trap_hook) ? rtai_trap_hook(exception, regs) : 0;
 }
 
-int __ipipe_notify_trap(int exception, struct pt_regs *regs)
+int __Ipipe_notify_trap(int exception, struct pt_regs *regs)
 {
 	struct ipipe_percpu_domain_data *p;
 	struct ipipe_trap_data data;
@@ -1155,7 +1168,6 @@ int __ipipe_notify_trap(int exception, struct pt_regs *regs)
 		hard_local_irq_restore(flags);
 		data.exception = exception;
 		data.regs = regs;
-		ret = ipipe_trap_hook(&data);
 		flags = hard_local_irq_save();
 		p->coflags &= ~__IPIPE_TRAP_R;
 	}
@@ -1172,12 +1184,12 @@ int __ipipe_notify_user_intreturn(void)
 	return !ipipe_root_p;
 }
 
-int __weak ipipe_kevent_hook(int kevent, void *data)
+int __ipipe_notify_kevent(int kevent, void *data)
 {
-	return 0;
+	ipipe_root_only(); return rtai_kevent_hook ? rtai_kevent_hook(kevent, data) : 0;
 }
 
-int __ipipe_notify_kevent(int kevent, void *data)
+int __Ipipe_notify_kevent(int kevent, void *data)
 {
 	struct ipipe_percpu_domain_data *p;
 	unsigned long flags;
@@ -1191,7 +1203,6 @@ int __ipipe_notify_kevent(int kevent, void *data)
 	if (likely(p->coflags & __IPIPE_KEVENT_E)) {
 		p->coflags |= __IPIPE_KEVENT_R;
 		hard_local_irq_restore(flags);
-		ret = ipipe_kevent_hook(kevent, data);
 		flags = hard_local_irq_save();
 		p->coflags &= ~__IPIPE_KEVENT_R;
 	}
@@ -1201,8 +1212,10 @@ int __ipipe_notify_kevent(int kevent, void *data)
 	return ret;
 }
 
-void __weak ipipe_migration_hook(struct task_struct *p)
+void inline ipipe_migration_hook(struct task_struct *p)
 {
+	if (rtai_migration_hook)
+		rtai_migration_hook(p);
 }
 
 static void complete_domain_migration(void) /* hw IRQs off */
@@ -1279,49 +1292,6 @@ void __ipipe_notify_vm_preemption(void)
 }
 EXPORT_SYMBOL_GPL(__ipipe_notify_vm_preemption);
 
-static void dispatch_irq_head(unsigned int irq) /* hw interrupts off */
-{
-	struct ipipe_percpu_domain_data *p = ipipe_this_cpu_head_context(), *old;
-	struct ipipe_domain *head = p->domain;
-
-	if (unlikely(test_bit(IPIPE_STALL_FLAG, &p->status))) {
-		__ipipe_set_irq_pending(head, irq);
-		return;
-	}
-
-	/* Switch to the head domain if not current. */
-	old = __ipipe_current_context;
-	if (old != p)
-		__ipipe_set_current_context(p);
-
-	p->irqall[irq]++;
-	__set_bit(IPIPE_STALL_FLAG, &p->status);
-	barrier();
-	head->irqs[irq].handler(irq, head->irqs[irq].cookie);
-	__ipipe_run_irqtail(irq);
-	hard_local_irq_disable();
-	p = ipipe_this_cpu_head_context();
-	__clear_bit(IPIPE_STALL_FLAG, &p->status);
-
-	/* Are we still running in the head domain? */
-	if (likely(__ipipe_current_context == p)) {
-		/* Did we enter this code over the head domain? */
-		if (old->domain == head) {
-			/* Yes, do immediate synchronization. */
-			if (__ipipe_ipending_p(p))
-				__ipipe_sync_stage();
-			return;
-		}
-		__ipipe_set_current_context(ipipe_this_cpu_root_context());
-	}
-
-	/*
-	 * We must be running over the root domain, synchronize
-	 * the pipeline for high priority IRQs (slow path).
-	 */
-	__ipipe_do_sync_pipeline(head);
-}
-
 void __ipipe_dispatch_irq(unsigned int irq, int flags) /* hw interrupts off */
 {
 	struct ipipe_domain *ipd;
@@ -1605,6 +1575,7 @@ void __ipipe_do_sync_stage(void)
 
 	__clear_bit(IPIPE_STALL_FLAG, &p->status);
 }
+EXPORT_SYMBOL_GPL(__ipipe_do_sync_stage);
 
 void __ipipe_call_mayday(struct pt_regs *regs)
 {
diff --git a/kernel/ipipe/timer.c b/kernel/ipipe/timer.c
index 22d5e5be874a..7826f99d73c3 100644
--- a/kernel/ipipe/timer.c
+++ b/kernel/ipipe/timer.c
@@ -566,9 +566,6 @@ void ipipe_timer_set(unsigned long cdelay)
 	 * extra call to the tick handler would simply occur after 4
 	 * billions ticks.
 	 */
-	if (cdelay > UINT_MAX)
-		cdelay = UINT_MAX;
-
 	tdelay = cdelay;
 	if (t->c2t_integ != 1)
 		tdelay *= t->c2t_integ;
@@ -579,7 +576,7 @@ void ipipe_timer_set(unsigned long cdelay)
 	if (tdelay > t->max_delay_ticks)
 		tdelay = t->max_delay_ticks;
 
-	if (t->set(tdelay, t->timer_set) < 0)
+	t->set(tdelay, t->timer_set); return; if (t->set(tdelay, t->timer_set) < 0)
 		ipipe_raise_irq(t->irq);
 }
 EXPORT_SYMBOL_GPL(ipipe_timer_set);
@@ -646,3 +643,8 @@ void __ipipe_timer_refresh_freq(unsigned int hrclock_freq)
 					  t->host_timer->next_event, false);
 	}
 }
+
+EXPORT_SYMBOL(ipipe_select_timers);
+EXPORT_SYMBOL(ipipe_timers_release);
+EXPORT_SYMBOL(ipipe_timer_start);
+EXPORT_SYMBOL(ipipe_timer_stop);
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 606664e090c2..5f5217bed4bc 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -86,6 +86,7 @@ int sysctl_sched_rt_runtime = 950000;
 
 /* CPUs with isolated domains */
 cpumask_var_t cpu_isolated_map;
+EXPORT_SYMBOL(cpu_isolated_map);
 
 /*
  * __task_rq_lock - lock the rq @p resides on.
@@ -2773,55 +2774,65 @@ asmlinkage __visible void schedule_tail(struct task_struct *prev)
 /*
  * context_switch - switch to the new MM and the new thread's register state.
  */
-static __always_inline struct rq *
+static struct rq *
 context_switch(struct rq *rq, struct task_struct *prev,
 	       struct task_struct *next, struct rq_flags *rf)
 {
 	struct mm_struct *mm, *oldmm;
 
-	prepare_task_switch(rq, prev, next);
-
 	mm = next->mm;
 	oldmm = prev->active_mm;
-	/*
-	 * For paravirt, this is coupled with an exit in switch_to to
-	 * combine the page table reload and the switch backend into
-	 * one hypercall.
-	 */
-	arch_start_context_switch(prev);
-
-	if (!mm) {
-		next->active_mm = oldmm;
-		mmgrab(oldmm);
-		enter_lazy_tlb(oldmm, next);
-	} else
-		switch_mm_irqs_off(oldmm, mm, next);
-
-	if (!prev->mm) {
-		prev->active_mm = NULL;
-		rq->prev_mm = oldmm;
-	}
 
-	rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);
+	if (likely(!rq)) {
+		switch_mm_irqs_off(oldmm, next->active_mm, next);
+		if (!mm) enter_lazy_tlb(oldmm, next);
+	} else {
+		prepare_task_switch(rq, prev, next);
 
-	/*
-	 * Since the runqueue lock will be released by the next
-	 * task (which is an invalid locking op but in the case
-	 * of the scheduler it's an obvious special-case), so we
-	 * do an early lockdep release here:
-	 */
-	rq_unpin_lock(rq, rf);
-	spin_release(&rq->lock.dep_map, 1, _THIS_IP_);
+		/*
+		 * For paravirt, this is coupled with an exit in switch_to to
+		 * combine the page table reload and the switch backend into
+		 * one hypercall.
+		 */
+		arch_start_context_switch(prev);
+
+		if (!mm) {
+			next->active_mm = oldmm;
+			mmgrab(oldmm);
+			enter_lazy_tlb(oldmm, next);
+		} else
+			switch_mm_irqs_off(oldmm, mm, next);
+
+		if (!prev->mm) {
+			prev->active_mm = NULL;
+			rq->prev_mm = oldmm;
+		}
+
+		rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);
+
+		/*
+		 * Since the runqueue lock will be released by the next
+		 * task (which is an invalid locking op but in the case
+		 * of the scheduler it's an obvious special-case), so we
+		 * do an early lockdep release here:
+		 */
+		rq_unpin_lock(rq, rf);
+		spin_release(&rq->lock.dep_map, 1, _THIS_IP_);
+	}
 
 	/* Here we just switch the register state and the stack. */
 	switch_to(prev, next, prev);
 	barrier();
 
+	if (likely(!rq))
+		return NULL;
+
 	if (unlikely(__ipipe_switch_tail()))
 		return NULL;
 
 	return finish_task_switch(prev);
 }
+EXPORT_SYMBOL(context_switch);
 
 /*
  * nr_running and nr_context_switches:
-- 
2.22.0.455.g172b71a6c5

